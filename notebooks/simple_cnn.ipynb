{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GpfdjqTZlDRQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae87442-5a64-40c9-fe3d-ae1c881803a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.29.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "MN4q_8mWLwF-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "X8ROGEdrLzpo",
        "outputId": "f25e949b-1944-49a8-a04a-af66e44a9805"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnipkha21\u001b[0m (\u001b[33mnipkha21-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4tTypGWrL1iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLbASSuzpV4J",
        "outputId": "3023c0ca-a538-4143-9546-d081b99ed896"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2"
      ],
      "metadata": {
        "id": "hE3VJyaPpjUU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from data_utils import load_split_data\n",
        "from models import SimpleCNN, get_model\n",
        "from training_utils import ModelTrainer, get_optimizer, get_scheduler\n",
        "from evaluation import ModelEvaluator"
      ],
      "metadata": {
        "id": "xBlXWvH_ppT_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "zonS4rtypyWZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNFacialExpressionDataset(Dataset):\n",
        "    def __init__(self, data, transform=None, augment=False, is_test=False):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        self.is_test = is_test\n",
        "\n",
        "        if transform is None:\n",
        "            if augment and not is_test:\n",
        "                self.transform = self._get_augmentation_transform()\n",
        "            else:\n",
        "                self.transform = self._get_basic_transform()\n",
        "\n",
        "    def _get_basic_transform(self):\n",
        "        return A.Compose([\n",
        "            A.Normalize(mean=[0.485], std=[0.229]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "    def _get_augmentation_transform(self):\n",
        "        return A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.Rotate(limit=15, p=0.3, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
        "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
        "            A.Blur(blur_limit=3, p=0.1),\n",
        "            A.CoarseDropout(max_holes=1, max_height=8, max_width=8,\n",
        "                           min_holes=1, min_height=4, min_width=4, p=0.2),\n",
        "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.3),\n",
        "            A.Normalize(mean=[0.485], std=[0.229]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pixels = self.data.iloc[idx]['pixels']\n",
        "        image = np.array([int(pixel) for pixel in pixels.split()]).reshape(48, 48)\n",
        "\n",
        "        image = np.stack([image] * 3, axis=-1).astype(np.uint8)\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "        else:\n",
        "            image = torch.FloatTensor(image).unsqueeze(0) / 255.0\n",
        "\n",
        "        if self.is_test:\n",
        "            return image\n",
        "        else:\n",
        "            emotion = int(self.data.iloc[idx]['emotion'])\n",
        "            return image, emotion"
      ],
      "metadata": {
        "id": "aynFXk90pvZ1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedSimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=7, dropout_rate=0.3, use_batch_norm=True):\n",
        "        super(ImprovedSimpleCNN, self).__init__()\n",
        "\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32) if use_batch_norm else nn.Identity()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64) if use_batch_norm else nn.Identity()\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128) if use_batch_norm else nn.Identity()\n",
        "\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256) if use_batch_norm else nn.Identity()\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((3, 3))\n",
        "\n",
        "        self.fc1 = nn.Linear(256 * 3 * 3, 512)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate / 2)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.shape[1] == 3:\n",
        "            x = x[:, 0:1, :, :]\n",
        "\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "\n",
        "        x = self.adaptive_pool(x)\n",
        "\n",
        "        x = x.view(-1, 256 * 3 * 3)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "aLZUH7CnpwLW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNTrainer:\n",
        "    def __init__(self, model, train_loader, val_loader, criterion, optimizer,\n",
        "                 scheduler=None, device='cuda', experiment_name='experiment', run_name='run'):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.device = device\n",
        "\n",
        "        wandb.init(\n",
        "            project=\"facial-expression-recognition\",\n",
        "            group=experiment_name,\n",
        "            name=run_name,\n",
        "            config={\n",
        "                \"data_split_method\": \"predefined_stratified\",\n",
        "                \"train_samples\": len(train_loader.dataset),\n",
        "                \"val_samples\": len(val_loader.dataset),\n",
        "                \"split_random_state\": 42\n",
        "            },\n",
        "            reinit=True\n",
        "        )\n",
        "        wandb.watch(self.model, log='all', log_freq=100)\n",
        "\n",
        "        self.history = {\n",
        "            'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [],\n",
        "            'learning_rates': [], 'epoch_times': []\n",
        "        }\n",
        "        self.best_val_acc = 0.0\n",
        "        self.best_model_state = None\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        pbar = tqdm(self.train_loader, desc=\"Training\")\n",
        "        for batch_idx, (data, targets) in enumerate(pbar):\n",
        "            data, targets = data.to(self.device), targets.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(data)\n",
        "            loss = self.criterion(outputs, targets)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'Acc': f'{100.*correct/total:.2f}%'\n",
        "            })\n",
        "\n",
        "        epoch_loss = running_loss / len(self.train_loader)\n",
        "        epoch_acc = 100. * correct / total\n",
        "\n",
        "        return epoch_loss, epoch_acc\n",
        "\n",
        "    def validate_epoch(self):\n",
        "        self.model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, targets in tqdm(self.val_loader, desc=\"Validation\"):\n",
        "                data, targets = data.to(self.device), targets.to(self.device)\n",
        "\n",
        "                outputs = self.model(data)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += targets.size(0)\n",
        "                correct += (predicted == targets).sum().item()\n",
        "\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "                all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "        epoch_loss = running_loss / len(self.val_loader)\n",
        "        epoch_acc = 100. * correct / total\n",
        "\n",
        "        return epoch_loss, epoch_acc, np.array(all_predictions), np.array(all_targets)\n",
        "\n",
        "    def train(self, epochs, early_stopping_patience=10):\n",
        "        print(f\"Starting CNN training for {epochs} epochs...\")\n",
        "\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_start = time.time()\n",
        "\n",
        "            train_loss, train_acc = self.train_epoch()\n",
        "\n",
        "            val_loss, val_acc, val_preds, val_targets = self.validate_epoch()\n",
        "\n",
        "            current_lr = self.optimizer.param_groups[0]['lr']\n",
        "            if self.scheduler:\n",
        "                if isinstance(self.scheduler, ReduceLROnPlateau):\n",
        "                    self.scheduler.step(val_loss)\n",
        "                else:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "            if val_acc > self.best_val_acc:\n",
        "                self.best_val_acc = val_acc\n",
        "                self.best_model_state = self.model.state_dict().copy()\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            epoch_time = time.time() - epoch_start\n",
        "\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "            self.history['train_acc'].append(train_acc)\n",
        "            self.history['val_loss'].append(val_loss)\n",
        "            self.history['val_acc'].append(val_acc)\n",
        "            self.history['learning_rates'].append(current_lr)\n",
        "            self.history['epoch_times'].append(epoch_time)\n",
        "\n",
        "            log_dict = {\n",
        "                'epoch': epoch + 1,\n",
        "                'train_loss': train_loss,\n",
        "                'train_accuracy': train_acc,\n",
        "                'val_loss': val_loss,\n",
        "                'val_accuracy': val_acc,\n",
        "                'learning_rate': current_lr,\n",
        "                'epoch_time': epoch_time,\n",
        "                'best_val_accuracy': self.best_val_acc\n",
        "            }\n",
        "\n",
        "            if (epoch + 1) % 15 == 0:\n",
        "                emotion_map = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
        "                log_dict['confusion_matrix'] = wandb.plot.confusion_matrix(\n",
        "                    probs=None, y_true=val_targets, preds=val_preds,\n",
        "                    class_names=list(emotion_map.values())\n",
        "                )\n",
        "\n",
        "            wandb.log(log_dict)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}:\")\n",
        "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "            print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "            print(f\"  LR: {current_lr:.6f}, Time: {epoch_time:.2f}s\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            if patience_counter >= early_stopping_patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        if self.best_model_state:\n",
        "            self.model.load_state_dict(self.best_model_state)\n",
        "            print(f\"Loaded best model with validation accuracy: {self.best_val_acc:.2f}%\")\n",
        "\n",
        "        return self.history"
      ],
      "metadata": {
        "id": "gLu3o2LnpwNj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()\n",
        "\n",
        "print(\"=== LOADING PRE-SPLIT DATA ===\")\n",
        "\n",
        "train_df, val_df, test_df = load_split_data('drive/MyDrive/data')\n",
        "\n",
        "emotion_map = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Eoz1K0NpwQI",
        "outputId": "cfff08cd-af87-4451-bf52-8e12b68b204d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LOADING PRE-SPLIT DATA ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== DATA AUGMENTATION EXPERIMENTS ===\")\n",
        "\n",
        "augmentation_strategies = {\n",
        "    'no_augmentation': {\n",
        "        'augment': False,\n",
        "        'description': 'No data augmentation'\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "O_laEmaRpwTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmentation_results = {}\n",
        "\n",
        "for aug_name, aug_config in augmentation_strategies.items():\n",
        "    print(f\"\\nTesting augmentation strategy: {aug_name}\")\n",
        "    print(f\"Description: {aug_config['description']}\")\n",
        "\n",
        "    if 'transform' in aug_config:\n",
        "        train_dataset = CNNFacialExpressionDataset(\n",
        "            train_df, transform=aug_config['transform'], augment=False\n",
        "        )\n",
        "        val_dataset = CNNFacialExpressionDataset(\n",
        "            val_df, transform=A.Compose([\n",
        "                A.Normalize(mean=[0.485], std=[0.229]),\n",
        "                ToTensorV2()\n",
        "            ]), augment=False\n",
        "        )\n",
        "    else:\n",
        "        train_dataset = CNNFacialExpressionDataset(\n",
        "            train_df, augment=aug_config['augment']\n",
        "        )\n",
        "        val_dataset = CNNFacialExpressionDataset(\n",
        "            val_df, augment=False\n",
        "        )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = SimpleCNN(num_classes=7, dropout_rate=0.5)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    trainer = CNNTrainer(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        experiment_name=\"Simple_CNN_Training\",\n",
        "        run_name=f\"Augmentation_{aug_name}\"\n",
        "    )\n",
        "\n",
        "    history = trainer.train(epochs=10, early_stopping_patience=5)\n",
        "\n",
        "    augmentation_results[aug_name] = {\n",
        "        'best_val_acc': trainer.best_val_acc,\n",
        "        'final_train_acc': history['train_acc'][-1],\n",
        "        'final_val_acc': history['val_acc'][-1],\n",
        "        'overfitting_score': history['train_acc'][-1] - history['val_acc'][-1]\n",
        "    }\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "    print(f\"Best validation accuracy: {trainer.best_val_acc:.2f}%\")\n",
        "\n",
        "print(\"\\n=== AUGMENTATION ANALYSIS ===\")\n",
        "best_augmentation = max(augmentation_results.keys(),\n",
        "                       key=lambda x: augmentation_results[x]['best_val_acc'])\n",
        "print(f\"Best augmentation strategy: {best_augmentation}\")\n",
        "\n",
        "for aug_name, results in augmentation_results.items():\n",
        "    print(f\"{aug_name}: {results['best_val_acc']:.2f}% \"\n",
        "          f\"(overfitting: {results['overfitting_score']:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JXE-phYJpwU7",
        "outputId": "4a9f53c4-a4e5-40f9-e683-14985422f4ed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing augmentation strategy: no_augmentation\n",
            "Description: No data augmentation\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250607_193051-imwo1g6m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nipkha21-free-university-of-tbilisi-/facial-expression-recognition/runs/imwo1g6m' target=\"_blank\">Augmentation_no_augmentation</a></strong> to <a href='https://wandb.ai/nipkha21-free-university-of-tbilisi-/facial-expression-recognition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nipkha21-free-university-of-tbilisi-/facial-expression-recognition' target=\"_blank\">https://wandb.ai/nipkha21-free-university-of-tbilisi-/facial-expression-recognition</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nipkha21-free-university-of-tbilisi-/facial-expression-recognition/runs/imwo1g6m' target=\"_blank\">https://wandb.ai/nipkha21-free-university-of-tbilisi-/facial-expression-recognition/runs/imwo1g6m</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting CNN training for 10 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 314/314 [02:12<00:00,  2.36it/s, Loss=1.5715, Acc=37.74%]\n",
            "Validation: 100%|██████████| 68/68 [00:12<00:00,  5.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10:\n",
            "  Train Loss: 1.5900, Train Acc: 37.74%\n",
            "  Val Loss: 1.4204, Val Acc: 44.79%\n",
            "  LR: 0.001000, Time: 145.54s\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 314/314 [02:09<00:00,  2.42it/s, Loss=1.2098, Acc=49.32%]\n",
            "Validation: 100%|██████████| 68/68 [00:13<00:00,  5.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10:\n",
            "  Train Loss: 1.3352, Train Acc: 49.32%\n",
            "  Val Loss: 1.2719, Val Acc: 51.36%\n",
            "  LR: 0.001000, Time: 143.25s\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 314/314 [02:18<00:00,  2.27it/s, Loss=1.3596, Acc=54.37%]\n",
            "Validation: 100%|██████████| 68/68 [00:13<00:00,  5.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10:\n",
            "  Train Loss: 1.2092, Train Acc: 54.37%\n",
            "  Val Loss: 1.2086, Val Acc: 53.82%\n",
            "  LR: 0.001000, Time: 151.95s\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 314/314 [02:17<00:00,  2.28it/s, Loss=1.0403, Acc=58.55%]\n",
            "Validation: 100%|██████████| 68/68 [00:11<00:00,  5.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10:\n",
            "  Train Loss: 1.1012, Train Acc: 58.55%\n",
            "  Val Loss: 1.1663, Val Acc: 55.91%\n",
            "  LR: 0.001000, Time: 149.52s\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 314/314 [02:17<00:00,  2.28it/s, Loss=0.9871, Acc=63.09%]\n",
            "Validation: 100%|██████████| 68/68 [00:12<00:00,  5.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10:\n",
            "  Train Loss: 0.9873, Train Acc: 63.09%\n",
            "  Val Loss: 1.1854, Val Acc: 56.37%\n",
            "  LR: 0.001000, Time: 150.34s\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 314/314 [02:17<00:00,  2.29it/s, Loss=0.9181, Acc=67.60%]\n",
            "Validation: 100%|██████████| 68/68 [00:13<00:00,  5.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10:\n",
            "  Train Loss: 0.8794, Train Acc: 67.60%\n",
            "  Val Loss: 1.1838, Val Acc: 56.74%\n",
            "  LR: 0.001000, Time: 150.23s\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 314/314 [02:17<00:00,  2.28it/s, Loss=0.6850, Acc=71.87%]\n",
            "Validation: 100%|██████████| 68/68 [00:13<00:00,  5.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10:\n",
            "  Train Loss: 0.7586, Train Acc: 71.87%\n",
            "  Val Loss: 1.2657, Val Acc: 56.68%\n",
            "  LR: 0.001000, Time: 150.87s\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 314/314 [02:16<00:00,  2.30it/s, Loss=0.8289, Acc=77.17%]\n",
            "Validation: 100%|██████████| 68/68 [00:13<00:00,  5.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10:\n",
            "  Train Loss: 0.6334, Train Acc: 77.17%\n",
            "  Val Loss: 1.3005, Val Acc: 56.40%\n",
            "  LR: 0.001000, Time: 149.76s\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 314/314 [02:16<00:00,  2.29it/s, Loss=0.4448, Acc=80.74%]\n",
            "Validation: 100%|██████████| 68/68 [00:11<00:00,  5.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10:\n",
            "  Train Loss: 0.5293, Train Acc: 80.74%\n",
            "  Val Loss: 1.3803, Val Acc: 57.21%\n",
            "  LR: 0.001000, Time: 148.49s\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 314/314 [02:16<00:00,  2.30it/s, Loss=0.2924, Acc=84.11%]\n",
            "Validation: 100%|██████████| 68/68 [00:13<00:00,  5.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10:\n",
            "  Train Loss: 0.4374, Train Acc: 84.11%\n",
            "  Val Loss: 1.4438, Val Acc: 57.05%\n",
            "  LR: 0.001000, Time: 149.70s\n",
            "--------------------------------------------------\n",
            "Loaded best model with validation accuracy: 57.21%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_accuracy</td><td>▁▅▆▇██████</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch_time</td><td>▃▁█▆▇▇▇▆▅▆</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▄▅▆▆▇▇█</td></tr><tr><td>train_loss</td><td>█▆▆▅▄▄▃▂▂▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇██████</td></tr><tr><td>val_loss</td><td>▇▄▂▁▁▁▄▄▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_accuracy</td><td>57.20919</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>epoch_time</td><td>149.69551</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>train_accuracy</td><td>84.11048</td></tr><tr><td>train_loss</td><td>0.43737</td></tr><tr><td>val_accuracy</td><td>57.04667</td></tr><tr><td>val_loss</td><td>1.44376</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Augmentation_no_augmentation</strong> at: <a href='https://wandb.ai/nipkha21-free-university-of-tbilisi-/facial-expression-recognition/runs/imwo1g6m' target=\"_blank\">https://wandb.ai/nipkha21-free-university-of-tbilisi-/facial-expression-recognition/runs/imwo1g6m</a><br> View project at: <a href='https://wandb.ai/nipkha21-free-university-of-tbilisi-/facial-expression-recognition' target=\"_blank\">https://wandb.ai/nipkha21-free-university-of-tbilisi-/facial-expression-recognition</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250607_193051-imwo1g6m/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best validation accuracy: 57.21%\n",
            "\n",
            "=== AUGMENTATION ANALYSIS ===\n",
            "Best augmentation strategy: no_augmentation\n",
            "no_augmentation: 57.21% (overfitting: 27.06%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmentation_results = {}\n",
        "\n",
        "for aug_name, aug_config in augmentation_strategies.items():\n",
        "    print(f\"\\nTesting augmentation strategy: {aug_name}\")\n",
        "    print(f\"Description: {aug_config['description']}\")\n",
        "\n",
        "    if 'transform' in aug_config:\n",
        "        train_dataset = CNNFacialExpressionDataset(\n",
        "            train_df, transform=aug_config['transform'], augment=False\n",
        "        )\n",
        "        val_dataset = CNNFacialExpressionDataset(\n",
        "            val_df, transform=A.Compose([\n",
        "                A.Normalize(mean=[0.485], std=[0.229]),\n",
        "                ToTensorV2()\n",
        "            ]), augment=False\n",
        "        )\n",
        "    else:\n",
        "        train_dataset = CNNFacialExpressionDataset(\n",
        "            train_df, augment=aug_config['augment']\n",
        "        )\n",
        "        val_dataset = CNNFacialExpressionDataset(\n",
        "            val_df, augment=False\n",
        "        )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = SimpleCNN(num_classes=7, dropout_rate=0.5)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    trainer = CNNTrainer(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        experiment_name=\"Simple_CNN_Training\",\n",
        "        run_name=f\"Augmentation_{aug_name}\"\n",
        "    )\n",
        "\n",
        "    history = trainer.train(epochs=10, early_stopping_patience=5)\n",
        "\n",
        "    augmentation_results[aug_name] = {\n",
        "        'best_val_acc': trainer.best_val_acc,\n",
        "        'final_train_acc': history['train_acc'][-1],\n",
        "        'final_val_acc': history['val_acc'][-1],\n",
        "        'overfitting_score': history['train_acc'][-1] - history['val_acc'][-1]\n",
        "    }\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "    print(f\"Best validation accuracy: {trainer.best_val_acc:.2f}%\")\n",
        "\n",
        "print(\"\\n=== AUGMENTATION ANALYSIS ===\")\n",
        "best_augmentation = max(augmentation_results.keys(),\n",
        "                       key=lambda x: augmentation_results[x]['best_val_acc'])\n",
        "print(f\"Best augmentation strategy: {best_augmentation}\")\n",
        "\n",
        "for aug_name, results in augmentation_results.items():\n",
        "    print(f\"{aug_name}: {results['best_val_acc']:.2f}% \"\n",
        "          f\"(overfitting: {results['overfitting_score']:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "OtNRp8MwpwXd",
        "outputId": "c6810fcc-b5fe-478b-e500-f575c7167c48"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing augmentation strategy: advanced_augmentation\n",
            "Description: Advanced: Flip + Rotation + Brightness + Noise\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250607_195828-ebbsju9z</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nipkha21-free-university-of-tbilisi-/facial-expression-recognition/runs/ebbsju9z' target=\"_blank\">Augmentation_advanced_augmentation</a></strong> to <a href='https://wandb.ai/nipkha21-free-university-of-tbilisi-/facial-expression-recognition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nipkha21-free-university-of-tbilisi-/facial-expression-recognition' target=\"_blank\">https://wandb.ai/nipkha21-free-university-of-tbilisi-/facial-expression-recognition</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nipkha21-free-university-of-tbilisi-/facial-expression-recognition/runs/ebbsju9z' target=\"_blank\">https://wandb.ai/nipkha21-free-university-of-tbilisi-/facial-expression-recognition/runs/ebbsju9z</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting CNN training for 10 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 314/314 [02:11<00:00,  2.38it/s, Loss=1.6199, Acc=31.11%]\n",
            "Validation: 100%|██████████| 68/68 [00:11<00:00,  5.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10:\n",
            "  Train Loss: 1.7157, Train Acc: 31.11%\n",
            "  Val Loss: 1.5453, Val Acc: 40.86%\n",
            "  LR: 0.001000, Time: 143.68s\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  84%|████████▍ | 263/314 [01:53<00:22,  2.31it/s, Loss=1.4774, Acc=39.37%]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-562241142760>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_patience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     augmentation_results[aug_name] = {\n",
            "\u001b[0;32m<ipython-input-11-fb69e78348c3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, early_stopping_patience)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-fb69e78348c3>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/integration/torch/wandb_torch.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(grad)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_tensor_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_track\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hook_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_augmentation = 'no_augmentation'  # es iyo itogshi sauketeso agar davrune bolomde :))"
      ],
      "metadata": {
        "id": "x7BvougV_Sbb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== CNN ARCHITECTURE COMPARISON ===\")\n",
        "\n",
        "architectures = {\n",
        "    'simple_cnn_low_dropout': {\n",
        "        'model': SimpleCNN(num_classes=7, dropout_rate=0.3),\n",
        "        'description': 'Basic CNN with lower dropout'\n",
        "    },\n",
        "    'improved_cnn': {\n",
        "        'model': ImprovedSimpleCNN(num_classes=7, dropout_rate=0.3, use_batch_norm=True),\n",
        "        'description': 'Improved CNN with BatchNorm'\n",
        "    },\n",
        "}\n",
        "\n",
        "architecture_results = {}\n",
        "\n",
        "best_aug_config = augmentation_strategies[best_augmentation]\n",
        "\n",
        "for arch_name, arch_config in architectures.items():\n",
        "    print(f\"\\nTesting architecture: {arch_name}\")\n",
        "    print(f\"Description: {arch_config['description']}\")\n",
        "\n",
        "    if 'transform' in best_aug_config:\n",
        "        train_dataset = CNNFacialExpressionDataset(\n",
        "            train_df, transform=best_aug_config['transform'], augment=False\n",
        "        )\n",
        "        val_dataset = CNNFacialExpressionDataset(\n",
        "            val_df, transform=A.Compose([\n",
        "                A.Normalize(mean=[0.485], std=[0.229]),\n",
        "                ToTensorV2()\n",
        "            ]), augment=False\n",
        "        )\n",
        "    else:\n",
        "        train_dataset = CNNFacialExpressionDataset(\n",
        "            train_df, augment=best_aug_config['augment']\n",
        "        )\n",
        "        val_dataset = CNNFacialExpressionDataset(\n",
        "            val_df, augment=False\n",
        "        )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = arch_config['model']\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    trainer = CNNTrainer(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        device=device,\n",
        "        experiment_name=\"Simple_CNN_Training\",\n",
        "        run_name=f\"Architecture_{arch_name}\"\n",
        "    )\n",
        "\n",
        "    history = trainer.train(epochs=10, early_stopping_patience=7)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    architecture_results[arch_name] = {\n",
        "        'best_val_acc': trainer.best_val_acc,\n",
        "        'final_train_acc': history['train_acc'][-1],\n",
        "        'final_val_acc': history['val_acc'][-1],\n",
        "        'total_params': total_params,\n",
        "        'trainable_params': trainable_params,\n",
        "        'avg_epoch_time': np.mean(history['epoch_times'])\n",
        "    }\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "    print(f\"Best validation accuracy: {trainer.best_val_acc:.2f}%\")\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "print(\"\\n=== ARCHITECTURE ANALYSIS ===\")\n",
        "best_architecture = max(architecture_results.keys(),\n",
        "                       key=lambda x: architecture_results[x]['best_val_acc'])\n",
        "print(f\"Best architecture: {best_architecture}\")\n",
        "\n",
        "for arch_name, results in architecture_results.items():\n",
        "    print(f\"{arch_name}: {results['best_val_acc']:.2f}% \"\n",
        "          f\"({results['total_params']:,} params, \"\n",
        "          f\"{results['avg_epoch_time']:.2f}s/epoch)\")"
      ],
      "metadata": {
        "id": "jCZ5WhNn_a_F",
        "outputId": "159aef70-5208-4466-d0e2-eda026d054eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CNN ARCHITECTURE COMPARISON ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'no_augmentation'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-90a063d2608d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0marchitecture_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mbest_aug_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmentation_strategies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_augmentation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0march_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march_config\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marchitectures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'no_augmentation'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wFqnCwSzBqa3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}